{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import math\n",
    "from string import punctuation\n",
    "from unidecode import unidecode\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import html\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import en_core_web_sm\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "DEV_PATH = 'DSL2122_january_dataset/development.csv'\n",
    "EVAL_PATH = 'DSL2122_january_dataset/evaluation.csv'\n",
    "\n",
    "N_JOBS = 12\n",
    "URL_REGEX = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "HASHTAG_REGEX = r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\n",
    "TARGET_REGEX = r\"(?:@[\\w_]+)\"\n",
    "CAPITALIZED_WORDS_REGEX = r\"\\b[A-Z]{2,}\\b\"\n",
    "\n",
    "EMOTICONS = {\n",
    "    ('smiley',): [':-)', ':-))', ':-)))', ':)', ':))', ':)))', ':-]', ':]', ':-3', ':3', ':->', ':>', '8-)', ':o)', ':-}', ':}', ':c)', ':^)', '=]', '=)'],\n",
    "    ('laugh',): [':-d', ':d', '8-d', '8d', 'x-d', 'xd', '=d', '=3', 'b^d', '>^_^<', '<^!^>', '^/^', '（*^_^*）', '(^<^) (^.^)', '(^^)', '(^.^)', '(^_^.)', '(^_^)', '(^j^)', '(*^.^*)', '(^—^）', '(#^.^#)', '(*^^)v', '(^_^)v'],\n",
    "    ('sad',): [':-(', ':(', ':-c', ':c', ':-<', ':<', ':-[', ':[', ':-||', '>:[', ':{', ':@', '>:(', \":'-)\", \":')\", \"('_')\", '(/_;)', '(t_t) (;_;)', '(;_;', '(;_:)', '(;o;)', '(:_;)', '(tot)', ';_;', ';-;', ';n;', ';;', 'q.q', 't.t', 'qq', 'q_q', '(＾v＾)', '(＾ｕ＾)', '(^)o(^)', '(^o^)', ')^o^('],\n",
    "    ('crying',): [\":'-(\", \":'(\"],\n",
    "    ('horror',): [\"d-':\"],\n",
    "    ('disgust',): ['d:<'],\n",
    "    ('playful',): ['d:', ':-p', ':p', 'x-p', 'xp', ':-þ', ':þ', ':b', '=p', '>:p', ':-b'],\n",
    "    ('dismay',): ['d8', 'd;', 'd=', 'dx'],\n",
    "    ('surprise',): [':-o', ':o'],\n",
    "    ('shock',): [':-0'],\n",
    "    ('yawn',): ['8-0', '>:o'],\n",
    "    ('kiss',): [':-*', ':*'],\n",
    "    ('tongue', 'tied'): [':x', ':-x', ':-#', ':#', ':-&', ':&'],\n",
    "    ('wink',): [';-)', ';)', '*-)', '*)', ';-]', ';]', ';^)', ':-,', ';d', '(^_-)'],\n",
    "    ('annoyed',): [':-/', ':/', ':-[.]', '>:[(\\\\)]', '>:/', ':[(\\\\)]', '=/', '=[(\\\\)]', ':l', '=l', ':s'],\n",
    "    ('straight',): [':-|', ':|'],\n",
    "    ('embarrassed',): [':$'],\n",
    "    ('angel',): ['o:-)', 'o:)', '0:-3', '0:3', '0:-)', '0:)', '0;^)'],\n",
    "    ('evil',): ['>:-)', '>:)', '}:-)', '}:)', '3:-)', '3:)', '>;)'],\n",
    "    ('cool',): ['|;-)'],\n",
    "    ('bored',): ['|-o'],\n",
    "    ('tongue',): [':-j'],\n",
    "    ('party',): ['#-)'],\n",
    "    ('drunk',): ['%-)', '%)'],\n",
    "    ('sick',): [':-###..', ':###..'],\n",
    "    ('dump',): ['<:-|'],\n",
    "    ('troubled',): ['(>_<)', '(>_<)>'],\n",
    "    ('baby',): [\"(';')\"],\n",
    "    ('shy',): ['(^^>``', '(^_^;)', '(-_-;)', '(~_~;) (・.・;)'],\n",
    "    ('sleeping',): ['(-_-)zzz'],\n",
    "    ('confused',): ['((+_+))', '(+o+)'],\n",
    "    ('ultraman',): ['(o|o)'],\n",
    "    ('joyful',): ['^_^', '(^_^)/', '(^o^)／'],\n",
    "    ('respect',): ['(__)', '_(._.)_', '<(_ _)>', '<m(__)m>', 'm(__)m', 'm(_ _)m'],\n",
    "    ('shame',): ['(-.-)', '(-_-)', '(一一)', '(；一_一)'],\n",
    "    ('tired',): ['(=_=)'],\n",
    "    ('cat',): ['(=^·^=)', '(=^··^=)', '=_^= '],\n",
    "    ('looking', 'down'): ['(..)', '(._.)'],\n",
    "    ('giggling',): ['^m^'],\n",
    "    ('confusion',): ['(・・?', '(?_?)'],\n",
    "    ('waving',): ['（^—^）', '(;_;)/~~~', '(^.^)/~~~', '(-_-)/~~~ ($··)/~~~', '(t_t)/~~~', '(tot)/~~~'],\n",
    "    ('excited',): ['(*^0^*)'],\n",
    "    ('amazed',): ['(*_*)', '(*_*;', '(+_+)', '(@_@)'],\n",
    "    ('music',): ['((d[-_-]b))'],\n",
    "    ('worried',): ['(-\"-)', '(ーー;)'],\n",
    "    ('eyeglasses',): ['(^0_0^)'],\n",
    "    ('surprised',): [':o o_o', 'o_0', '(o.o)', 'oo'],\n",
    "    ('surpised',): ['o.o'],\n",
    "    ('dissatisfied',): ['(*￣m￣)'],\n",
    "    ('deflated',): [\"('a`)\"],\n",
    "}\n",
    "\n",
    "EMOTICONS_DICT = {}\n",
    "for k, v in EMOTICONS.items():\n",
    "    for emo in v:\n",
    "        EMOTICONS_DICT[emo] = list(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # https://pythonspot.com/nltk-stop-words/\n",
    "    # https://pythonspot.com/nltk-stemming/\n",
    "    text = unidecode(text)\n",
    "#   stopWords = set(stopwords.words('english'))\n",
    "    # tk = word_tokenize\n",
    "    tk = TweetTokenizer().tokenize\n",
    "    words = tk(text)\n",
    "    wordsCleaned = []\n",
    "    ps = PorterStemmer()\n",
    "    for word0 in words:\n",
    "        no_emoticons = EMOTICONS_DICT.get(word0, [word0])\n",
    "        for word in no_emoticons:\n",
    "            ww = word_tokenize(word)\n",
    "            if len(ww) > 1 and any([x in punctuation for x in [ww[0][-1], ww[1][0]]]):\n",
    "                ww = [word]\n",
    "            # print(ww)\n",
    "            for w in ww:\n",
    "                sw = ps.stem(w)\n",
    "                # print(w, sw)\n",
    "                # if len(sw) > 1 and sw not in stopWords: # wanted also to delete len(sw) > 1\n",
    "                wordsCleaned.append(sw)\n",
    "    return wordsCleaned\n",
    "\n",
    "\n",
    "def tokenize_texts_it(args):\n",
    "    (i, text, args) = args\n",
    "    return tokenize(text)\n",
    "\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return parallel(tokenize_texts_it, texts)\n",
    "    # return [tokenize(text) for text in texts]\n",
    "\n",
    "\n",
    "# twitter tokenized, w/o stop words, stemmed, tokenized, ...\n",
    "def wordset(tokenized_texts):\n",
    "    words = {}\n",
    "    for tokenized_text in tokenized_texts:\n",
    "        for token in tokenized_text:\n",
    "            words[token] = words.get(token, 0) + 1\n",
    "    return words\n",
    "\n",
    "# sentiments is an iterable of integers in {0,1}\n",
    "\n",
    "\n",
    "def ngrams(texts, words, sentiments=None):\n",
    "    regex = re.compile(r'((.)\\2{2,})')\n",
    "    neg = set((\"not\", \"nor\", \"n't\", \"no\"))\n",
    "    stop_neg = set(\"!),-.;>?]_}\")\n",
    "    stop_j = [\"but\", \"so\", \"after\", \"before\", \"although\", \"even\", \"because\", \"as\", \"if\", \"till\", \"until\", \"unless\", \"once\", \"while\", \"whereas\", \"spite\", \"despite\", \"addition\", \"furthermore\", \"however\", \"anyway\",\n",
    "              \"therefore\", \"consequently\", \"though\", \"provided\", \"since\", \"whenever\", \"wherever\", \"besides\", \"conversely\", \"later\", \"moreover\", \"nonetheless\", \"thereafter\", \"thus\", \"whatever\", \"whoever\", \"cause\"]\n",
    "    stop_j = [tokenize(w)[0] for w in stop_j]\n",
    "    # non_stop_j = [\"and\", \"or\", \"nor\", \"where\", \"when\", \"still\", \"yet\"]\n",
    "    stop_neg.update(stop_j)\n",
    "    ngram_features = []\n",
    "    all_weights = []\n",
    "    last_words = []\n",
    "    # sent_count = {k:0 for k in ['0','1']}\n",
    "    sent_count = [0, 0]\n",
    "    if sentiments is None:\n",
    "        sentiments = [0]*len(texts)\n",
    "    for sentiment, text in tqdm(list(zip(sentiments, texts))):  # single tweet\n",
    "        text_weights = []\n",
    "        ngram_feature = {}\n",
    "        lw = ('', 0)\n",
    "        # for tk in split_list(text): # subtweet\n",
    "        # for subtext in sent_tokenize(text):  # subtweet\n",
    "        for subtext in [text]:\n",
    "            tk = tokenize(subtext)\n",
    "            ls = []\n",
    "            signs = []\n",
    "            sign = 1\n",
    "            for s in tk:\n",
    "                if s in neg:\n",
    "                    sign = -1\n",
    "                else:\n",
    "                    if s in stop_neg:\n",
    "                        sign = 1\n",
    "                    if len(s) > 1:\n",
    "                        ls.append(s)\n",
    "                        signs.append(sign)\n",
    "            # sign = -1 if any([\"not\" in s or \"n't\" in s for s in tk]) else 1\n",
    "            sent_count[sentiment] += 1\n",
    "            if len(ls) == 0:\n",
    "                continue\n",
    "            # sent_count[sentiment if sign == 1 else 1-sentiment] += 1\n",
    "            # subt = ' '.join(tk).replace(\"not\",\"\").replace(\"n't\",\"\")\n",
    "            # remove stop words and stem subt, tokenize\n",
    "            # ls = subt.split() # ls is the list obtained tokenizing subt\n",
    "            weights = []\n",
    "            for i, (s, sign) in enumerate(zip(ls, signs)):\n",
    "                matches = [x[0] for x in regex.findall(s)]\n",
    "                if len(matches) > 0:\n",
    "                    weights.append(1.5*sign)\n",
    "                    max_occ = 0\n",
    "                    for cc in itertools.product((1, 2), repeat=len(matches)):\n",
    "                        ts = s\n",
    "                        for m, c in zip(matches, cc):\n",
    "                            ts = ts.replace(m, m[:c], 1)\n",
    "                        occ = words.get(ts, 0)\n",
    "                        if max_occ < occ:\n",
    "                            max_occ = occ\n",
    "                            w = ts\n",
    "                    if max_occ == 0:\n",
    "                        ls[i] = ts\n",
    "                    else:\n",
    "                        ls[i] = w\n",
    "                else:\n",
    "                    weights.append(sign)\n",
    "            lw = (ls[-1], weights[-1])\n",
    "            for i in range(1, 3):  # 1 and 2 -grams\n",
    "                for j in range(0, len(ls)+i-1):\n",
    "                    w = '-'.join(ls[j:j+i])\n",
    "                    # use average words' weight for n-gram weights\n",
    "                    ngram_feature[w] = ngram_feature.get(\n",
    "                        w, 0) + sum(weights[j:j+i])/i\n",
    "            text_weights.extend(weights)\n",
    "        ngram_features.append(ngram_feature)\n",
    "        last_words.append(lw)\n",
    "        all_weights.append(text_weights)\n",
    "    return ngram_features, all_weights, last_words, sent_count\n",
    "\n",
    "\n",
    "def nlogn(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    return n*math.log2(n)\n",
    "\n",
    "\n",
    "def so_ne_it(so, ne, freq, freq_sent, sp, w, fw):\n",
    "    p1w = freq_sent[1].get(w, 0)/fw\n",
    "    p0w = freq_sent[0].get(w, 0)/fw\n",
    "    # SO(w) = log(p(+|w)/p(+)) - log(p(-|w)/p(-))\n",
    "    # so[w] = math.log2(p1w/sp[1]) - math.log2(p0w/sp[0])\n",
    "    if p1w/sp[1] > p0w/sp[0]:\n",
    "        so[w] = 1\n",
    "    else:\n",
    "        so[w] = -1\n",
    "    # ne(w) = 1 − (−(p(+|w).log(p(+|w)) − p(−|w).log(p(−|w))))\n",
    "    ne[w] = (1 + nlogn(p1w) + nlogn(p0w))  # * math.log(freq[w])\n",
    "\n",
    "\n",
    "def ngram_pop(to_remove, d):\n",
    "    for k in list(d.keys()):\n",
    "        if k in to_remove:\n",
    "            d.pop(k)\n",
    "\n",
    "\n",
    "def so_ne(sentiments, ngram_features, sent_count):\n",
    "    freq = {}\n",
    "    freq_sent = [{}, {}]\n",
    "    so = {}\n",
    "    ne = {}\n",
    "    for sent, ngrams in list(zip(sentiments, ngram_features)):\n",
    "        for k, v in ngrams.items():\n",
    "            freq[k] = freq.get(k, 0) + abs(v)\n",
    "            tsent = sent if v > 0 else 1-sent\n",
    "            freq_sent[tsent][k] = freq_sent[tsent].get(k, 0) + abs(v)\n",
    "    to_remove = set()\n",
    "    for k, v in freq.items():\n",
    "        if v < 3:\n",
    "            freq_sent[0].pop(k, 0)\n",
    "            freq_sent[1].pop(k, 0)\n",
    "            to_remove.add(k)\n",
    "    for k in to_remove:\n",
    "        freq.pop(k)\n",
    "    for d in ngram_features:\n",
    "        ngram_pop(to_remove, d)\n",
    "    c = sent_count[0] + sent_count[1]\n",
    "    sp = [sent_count[0]/c, sent_count[1]/c]\n",
    "    for w, fw in freq.items():\n",
    "        so_ne_it(so, ne, freq, freq_sent, sp, w, fw)\n",
    "    return so, ne, freq, freq_sent\n",
    "\n",
    "\n",
    "def lex_features_it(ngrams, lw, so, ne):\n",
    "    # number of positives and negatives, sum of postive scores, sum of abs of negative scores\n",
    "    np = nn = sp = sn = 0\n",
    "    for w, ww in ngrams.items():\n",
    "        if so.get(w, 0)*ww > 0:\n",
    "            np += abs(ww)\n",
    "            # sp += so[w]*ww # so based\n",
    "            sp += ne.get(w, 0)*abs(ww)  # so+ne based\n",
    "        elif so.get(w, 0)*ww < 0:\n",
    "            nn += abs(ww)\n",
    "            # sn -= so[w]*ww # so based\n",
    "            sn += ne.get(w, 0)*abs(ww)  # so+ne based\n",
    "    try:\n",
    "        lwp = ne.get(lw[0], 0)*so.get(lw[0], 0) * \\\n",
    "            lw[1]  # last word polarity (as score)\n",
    "    except:\n",
    "        print(type(lw))\n",
    "        print(lw)\n",
    "        raise IndexError()\n",
    "    return (np, nn, sp, sn, lwp)\n",
    "\n",
    "\n",
    "def lex_features(ngram_features, last_words, so, ne):\n",
    "    res = []\n",
    "    for ngrams, lw in list(zip(ngram_features, last_words)):\n",
    "        res.append(lex_features_it(ngrams, lw, so, ne))\n",
    "    return res\n",
    "\n",
    "\n",
    "def parallel(target, iterabile, args=(), n_jobs=N_JOBS):\n",
    "    l = len(iterabile)\n",
    "    iargs = zip(range(l), iterabile, [args]*l)\n",
    "    with Pool(processes=n_jobs) as executor:\n",
    "        res = list(executor.imap(target, iargs))\n",
    "    return res\n",
    "\n",
    "\n",
    "def tweet_word_sent(tweet_word, so, ne):\n",
    "    res = []\n",
    "    ps = PorterStemmer()\n",
    "    no_emoticons = EMOTICONS_DICT.get(tweet_word, [tweet_word])\n",
    "    for word in no_emoticons:\n",
    "        ww = word_tokenize(word)\n",
    "        if len(ww) > 1 and any([x in punctuation for x in [ww[0][-1], ww[1][0]]]):\n",
    "            ww = [word]\n",
    "        # print(ww)\n",
    "        for w in ww:\n",
    "            sw = ps.stem(w)\n",
    "            # print(w, sw)\n",
    "            # if len(sw) > 1 and sw not in stopWords: # wanted also to delete len(sw) > 1\n",
    "            res.append(sw)\n",
    "    # s = np.sign(sum([so.get(w,0) for w in res if not np.isclose(0, ne.get(w,0))]))\n",
    "    s = sum([so.get(w, 0)*ne.get(w, 0) for w in res])\n",
    "    return s\n",
    "\n",
    "\n",
    "def countable_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts countable features from text and adds those as columns in the DataFrame.\n",
    "    Countable features are:\n",
    "    - number of urls\n",
    "    - number of hashtags (#)\n",
    "    - number of targets (@)\n",
    "    - number of capitalized words (uppercase words having length > 1)    \n",
    "    - number of question marks\n",
    "    - number of esclamation marks\n",
    "    - number of quotes\n",
    "    - tweet length : number of words in the tweet\n",
    "    - avg word length\n",
    "    The text is then modified as follows:\n",
    "    - html entities unescaped (before feature extraction)\n",
    "    - urls are removed\n",
    "    - hashtags symbols # are removed, mainteining only the subsequent words\n",
    "    - targets are removed\n",
    "    - space characters are compressed\n",
    "    - leading whitespaces removed\n",
    "    - the entire text is converted to lowercase\n",
    "    \"\"\"\n",
    "    urls_list = []\n",
    "    hashtags_list = []\n",
    "    targets_list = []\n",
    "    capitalized_words_list = []\n",
    "    tweet_len_list = []\n",
    "    avg_word_len_list = []\n",
    "    question_marks_list = []\n",
    "    esclamation_marks_list = []\n",
    "    quotes_list = []\n",
    "    new_rows = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        row['text'] = html.unescape(row['text'])\n",
    "        urls = [''.join(i) for i in re.findall(URL_REGEX, row['text'])]\n",
    "        hashtags = re.findall(HASHTAG_REGEX, row['text'])\n",
    "        targets = re.findall(TARGET_REGEX, row['text'])\n",
    "        capitalized_words = re.findall(CAPITALIZED_WORDS_REGEX, row['text'])\n",
    "        question_marks = row['text'].count('?')\n",
    "        esclamation_marks = row['text'].count('!')\n",
    "        quotes = row['text'].count('\"') / 2\n",
    "\n",
    "        for token in urls + targets + ['#']:\n",
    "            row['text'] = row['text'].replace(token, '')\n",
    "        new_row = re.sub('\\s+', ' ', row['text']\n",
    "                         ).lower().strip(string.whitespace)\n",
    "\n",
    "        text_split = new_row.split()\n",
    "\n",
    "        tweet_len = len(text_split)\n",
    "        avg_word_len = np.mean([len(w)\n",
    "                               for w in text_split]) if text_split else 0\n",
    "\n",
    "        urls_list.append(len(urls))\n",
    "        hashtags_list.append(len(hashtags))\n",
    "        targets_list.append(len(targets))\n",
    "        capitalized_words_list.append(len(capitalized_words))\n",
    "        tweet_len_list.append(tweet_len)\n",
    "        avg_word_len_list.append(avg_word_len)\n",
    "        question_marks_list.append(question_marks)\n",
    "        esclamation_marks_list.append(esclamation_marks)\n",
    "        quotes_list.append(quotes)\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "    df['#urls'] = urls_list\n",
    "    df['#hashtags'] = hashtags_list\n",
    "    df['#targets'] = targets_list\n",
    "    df['#capitalized_words'] = capitalized_words_list\n",
    "    df['#words'] = tweet_len_list\n",
    "    df['avg_words_length'] = avg_word_len_list\n",
    "    df['#question_marks'] = question_marks_list\n",
    "    df['#esclamation_marks'] = esclamation_marks_list\n",
    "    df['#quotes'] = quotes_list\n",
    "    df['text'] = new_rows\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def pos_features(df: pd.Series, so, ne) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    return:\n",
    "    (so_sum(NOUN), so_sum(ADJ), so_sum(VERB), so_sum(INTJ), so_sum(ADJ),\n",
    "    sone_sum(NOUN), sone_sum(ADJ), sone_sum(VERB), sone_sum(INTJ), sone_sum(ADJ))\n",
    "    \"\"\"\n",
    "    nlp = en_core_web_sm.load()\n",
    "    POS_FEATURES = ['NOUN', 'ADJ', 'VERB', 'ADV', 'INTJ']\n",
    "    features = []\n",
    "\n",
    "    for i, text in tqdm(df.iteritems()):\n",
    "        doc = nlp(text)\n",
    "        # pos = [[t.text for t in doc if t.pos_ == feature]\n",
    "        #        for feature in POS_FEATURES]\n",
    "        #pos_numbers = tuple([len(pos[i]) for i in range(len(POS_FEATURES))])\n",
    "        #pos_percentages = tuple([i/len(words) for i in pos_numbers]) if len(words) else (0, 0, 0, 0, 0)\n",
    "        # pos_sone = [[tweet_word_sent(t, so, ne) for t in pos[i]]\n",
    "        #             for i in range(len(POS_FEATURES))]\n",
    "\n",
    "        pos_sone = [[tweet_word_sent(t.text, so, ne) for t in doc if t.pos_ == feature]\n",
    "               for feature in POS_FEATURES]\n",
    "\n",
    "        pos_sone_scores = tuple([sum(i) for i in pos_sone])\n",
    "        pos_so_scores = tuple([sum([np.sign(i) for i in p]) for p in pos_sone])\n",
    "\n",
    "        features.append(pos_so_scores + pos_sone_scores)\n",
    "\n",
    "    columns = [f'so_sum({p})' for p in POS_FEATURES] + \\\n",
    "        [f'sone_sum({p})' for p in POS_FEATURES]\n",
    "\n",
    "    new_df = pd.DataFrame(features, index=df.index, columns=columns)\n",
    "    return new_df\n",
    "\n",
    "def user_features(df: pd.DataFrame, transformer: pd.DataFrame=None, default_values: list=None):\n",
    "    \"\"\"\n",
    "    df: train DataFrame\n",
    "    transformer: DataFrame which maps users to #tweets and avg_sentiment. Evaluated only the first time using train df and used to transform the target df\n",
    "    default_values: default values of #tweets and avg_sentiment. Evaluated only once as for transformer\n",
    "    \"\"\"\n",
    "    if transformer is None:\n",
    "        transformer = pd.pivot_table(data=df, index='user', values='sentiment', aggfunc=['count', 'mean'])\n",
    "        transformer.columns = ['#tweets', 'avg_sentiment']\n",
    "    \n",
    "    if default_values is None:\n",
    "        avg_dataset_sentiment = df['sentiment'].sum()/len(df)\n",
    "        default_values = [0, avg_dataset_sentiment]\n",
    "\n",
    "    f8 = df['user'].map(lambda x: transformer.loc[x].values if x in transformer.index else default_values)\n",
    "    df['#tweets'] = [t[0] for t in f8]\n",
    "    df['avg_sentiment'] = [t[1] for t in f8]\n",
    "    df.drop('user', axis=1, inplace=True)\n",
    "\n",
    "    return df, transformer, default_values\n",
    "\n",
    "class LexiconExtractor:\n",
    "    \"\"\"\n",
    "    Extracts custom lexicon from train dataframe and extracts lexicon features \n",
    "    \"\"\"\n",
    "    lex_columns_names = ['#positive_ngrams', '#negative_ngrams', 'sum_positive_entropies', 'sum_negative_entropies', 'last_word_polarity']\n",
    "    \n",
    "    def __init__(self, df_fit_transform, sentiments, df_transform):\n",
    "        \"\"\"\n",
    "        Constructs lexicon, extracts features, transforms dataframes\n",
    "        \"\"\"\n",
    "        tokenized = tokenize_texts(df_fit_transform['text'])\n",
    "        self.words = wordset(tokenized)        \n",
    "        self.ngram_features, self.all_weights, self.last_words, self.sent_count = ngrams(texts=df_fit_transform['text'], words=self.words, sentiments=sentiments)\n",
    "        self.so, self.ne, self.freq, self.freq_sent = so_ne(sentiments, self.ngram_features, self.sent_count)        \n",
    "        self.lex = lex_features(self.ngram_features, self.last_words, self.so, self.ne)\n",
    "        df_fit_transform[self.lex_columns_names] = self.lex\n",
    "        \n",
    "        ngram_features_tr, all_weights_tr, last_words_tr, sent_count_tr = ngrams(texts=df_transform['text'], words=self.words)        \n",
    "        lex_tr = lex_features(ngram_features_tr, last_words_tr, self.so, self.ne)\n",
    "        df_transform[self.lex_columns_names] = lex_tr\n",
    "        \n",
    "        self.df1 = df_fit_transform\n",
    "        self.df2 = df_transform\n",
    "\n",
    "    def get_DataFrames(self):\n",
    "        \"\"\"\n",
    "        Get transformed dataframes\n",
    "        \"\"\"\n",
    "        return self.df1, self.df2\n",
    "    \n",
    "    def get_so_ne(self):\n",
    "        return self.so, self.ne\n",
    "\n",
    "    def get_freq(self):\n",
    "        return self.freq\n",
    "\n",
    "    def get_words(self):\n",
    "        return self.words\n",
    "\n",
    "def add_pos_features(df, df_text, so, ne):\n",
    "    \"\"\"\n",
    "    Wrapper for pos_features\n",
    "    \"\"\"\n",
    "    df_pos = pos_features(df_text, so, ne)\n",
    "    df = pd.concat([df, df_pos], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def date_to_ts(df):\n",
    "    \"\"\"\n",
    "    Converts dates to timestamps\n",
    "    \"\"\"\n",
    "    df['date'] = df['date'].map(lambda str: ' '.join(str.replace('PDT', '').split()[1:5]))\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['timestamp'] = df['date'].values.astype(np.int64)\n",
    "    df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(DEV_PATH, 'r') as f:\n",
    "\tdf_dev = pd.read_csv(f, usecols=[0, 2, 4, 5])\n",
    "\n",
    "with open(EVAL_PATH, 'r') as f:\n",
    "\tdf_eval = pd.read_csv(f, usecols=[1, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.drop_duplicates(subset=['text', 'sentiment'], keep='first', inplace=True, ignore_index=True)\n",
    "df_dev.drop_duplicates(subset='text', keep=False, inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter-specific and Textual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = countable_features(df_dev)\n",
    "df_eval = countable_features(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Validation, Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_valid, df_test = train_test_split(df_dev, train_size=.8, test_size=.2, shuffle=True, random_state=42)\n",
    "df_train, df_valid = train_test_split(df_train_valid, test_size=.25, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-textual (based on user) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, user_map, default_values = user_features(df_train) # fit\n",
    "df_valid, _, _ = user_features(df_valid, user_map, default_values) # transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_train = df_train.iloc[:, 0]\n",
    "df_train = df_train.iloc[:, 1:]\n",
    "\n",
    "sentiments_valid = df_valid.iloc[:, 0]\n",
    "df_valid = df_valid.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexicon-Based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = LexiconExtractor(df_train, sentiments_train, df_valid)\n",
    "\n",
    "df_train, df_valid = lexicon.get_DataFrames()\n",
    "so, ne = lexicon.get_so_ne()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save & Drop Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text = df_train['text']\n",
    "df_valid_text = df_valid['text']\n",
    "df_train.drop(['text'], axis=1, inplace=True)\n",
    "df_valid.drop(['text'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PoS-oriented features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = add_pos_features(df_train, df_train_text, so, ne)\n",
    "df_valid = add_pos_features(df_valid, df_valid_text, so, ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_docs = [' '.join([t for t, s in zip(df_train_text, sentiments_train) if s==i]) for i in [0, 1]]\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=123, binary=False)\n",
    "tfidf.fit(train_docs)\n",
    "vocab = tfidf.vocabulary_\n",
    "\n",
    "tfidf_train = tfidf.transform(df_train_text)\n",
    "tfidf_valid = tfidf.transform(df_valid_text)\n",
    "\n",
    "df_train = pd.concat([df_train, pd.DataFrame.sparse.from_spmatrix(tfidf_train, index=df_train.index, columns=sorted(vocab.keys()))], axis=1)\n",
    "df_valid = pd.concat([df_valid, pd.DataFrame.sparse.from_spmatrix(tfidf_valid, index=df_valid.index, columns=sorted(vocab.keys()))], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-textual (timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df_train = date_to_ts(df_train)\n",
    "df_valid = date_to_ts(df_valid)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_train.loc[:, ['timestamp']])\n",
    "\n",
    "df_train.loc[:, ['timestamp']] = scaler.transform(df_train.loc[:, ['timestamp']])\n",
    "df_valid.loc[:, ['timestamp']] = scaler.transform(df_valid.loc[:, ['timestamp']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "classifiers = [\n",
    "    RandomForestClassifier(n_jobs=N_JOBS),\n",
    "    BaggingClassifier(base_estimator=SVC(), max_samples=.25, max_features=.25, n_estimators=N_JOBS*2, n_jobs=N_JOBS),\n",
    "    GaussianNB(),\n",
    "    Pipeline([('scaler', MinMaxScaler()), ('knn', KNeighborsClassifier(n_jobs=N_JOBS))])] # KNN with normalized features\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf.fit(df_train, sentiments_train)\n",
    "    sentiments_pred = clf.predict(df_valid)\n",
    "    f1 = f1_score(sentiments_valid, sentiments_pred, average='macro')\n",
    "    print(f'{f1} : {clf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Pipeline([('scaler', MinMaxScaler()), ('knn', KNeighborsClassifier(n_jobs=N_JOBS, weights='distance'))])\n",
    "\n",
    "x.fit(df_train, sentiments_train)\n",
    "sentiments_pred = x.predict(df_valid)\n",
    "f1 = f1_score(sentiments_valid, sentiments_pred, average='macro')\n",
    "print(f'{f1} : {clf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ['gini', 'entropy']\n",
    "max_features = [1, 2, 3, 4, 5, 'auto', 'log2']\n",
    "min_samples_split = [2, 3, 4, 5, 6]\n",
    "\n",
    "for f in max_features:\n",
    "    for s in min_samples_split:\n",
    "        for c in criterion:\n",
    "            rf_tuning = RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                random_state=42,\n",
    "                max_features=f,\n",
    "                criterion=c,\n",
    "                min_samples_split=s,\n",
    "                n_jobs=N_JOBS,\n",
    "            )\n",
    "\n",
    "            rf_tuning.fit(df_train, sentiments_train)\n",
    "            pred_tuning = rf_tuning.predict(df_valid)\n",
    "            score = f1_score(sentiments_valid, pred_tuning, average='macro')\n",
    "\n",
    "            print(f'criterion={c}\\tmax_features={f}\\tmin_samples_split={s}\\tf1_score={score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=vocab, axis=1, inplace=True)\n",
    "df_valid.drop(columns=vocab, axis=1, inplace=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_docs = [' '.join([t for t, s in zip(df_train_text, sentiments_train) if s==i]) for i in [0, 1]]\n",
    "\n",
    "\n",
    "for i in [50, 75, 100, 125, 150, 175, 200]:\n",
    "    tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=i, binary=False)\n",
    "    tfidf.fit(train_docs)\n",
    "    vocab = tfidf.vocabulary_\n",
    "\n",
    "    tfidf_train = tfidf.transform(df_train_text)\n",
    "    tfidf_valid = tfidf.transform(df_valid_text)\n",
    "\n",
    "    tfidf_train = pd.concat([df_train, pd.DataFrame.sparse.from_spmatrix(tfidf_train, index=df_train.index, columns=sorted(vocab.keys()))], axis=1)\n",
    "    tfidf_valid = pd.concat([df_valid, pd.DataFrame.sparse.from_spmatrix(tfidf_valid, index=df_valid.index, columns=sorted(vocab.keys()))], axis=1)\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                random_state=42,\n",
    "                max_features=5,\n",
    "                criterion='gini',\n",
    "                min_samples_split=5,\n",
    "                n_jobs=N_JOBS,\n",
    "            )\n",
    "    rf.fit(tfidf_train, sentiments_train)\n",
    "    pred = rf.predict(tfidf_valid)\n",
    "\n",
    "    print(f\"n_features={i}\\tf1={f1_score(sentiments_valid, pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = sorted({(k, v) for k, v in zip(rf_tuning.feature_names_in_, rf_tuning.feature_importances_)}, key=lambda x: x[1], reverse=True)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_valid # use 80% of development set to test the classifier\n",
    "\n",
    "# user features\n",
    "df_train, user_map, default_values = user_features(df_train) # fit\n",
    "df_test, _, _ = user_features(df_test, user_map, default_values) # transform\n",
    "\n",
    "sentiments_train = df_train.iloc[:, 0]\n",
    "df_train = df_train.iloc[:, 1:]\n",
    "sentiments_test = df_test.iloc[:, 0]\n",
    "df_test = df_test.iloc[:, 1:]\n",
    "\n",
    "# lexicon features\n",
    "lexicon = LexiconExtractor(df_train, sentiments_train, df_test)\n",
    "df_train, df_test = lexicon.get_DataFrames()\n",
    "so, ne = lexicon.get_so_ne()\n",
    "\n",
    "df_train_text = df_train['text']\n",
    "df_test_text = df_test['text']\n",
    "df_train.drop(['text'], axis=1, inplace=True)\n",
    "df_test.drop(['text'], axis=1, inplace=True)\n",
    "\n",
    "# pos oriented features\n",
    "df_train = add_pos_features(df_train, df_train_text, so, ne)\n",
    "df_test = add_pos_features(df_test, df_test_text, so, ne)\n",
    "\n",
    "# BoW features\n",
    "train_docs = [' '.join([t for t, s in zip(df_train_text, sentiments_train) if s==i]) for i in [0, 1]]\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=123, binary=False)\n",
    "tfidf.fit(train_docs)\n",
    "vocab = tfidf.vocabulary_\n",
    "tfidf_train = tfidf.transform(df_train_text)\n",
    "tfidf_test = tfidf.transform(df_test_text)\n",
    "df_train = pd.concat([df_train, pd.DataFrame.sparse.from_spmatrix(tfidf_train, index=df_train.index, columns=sorted(vocab.keys()))], axis=1)\n",
    "df_test = pd.concat([df_test, pd.DataFrame.sparse.from_spmatrix(tfidf_test, index=df_test.index, columns=sorted(vocab.keys()))], axis=1)\n",
    "\n",
    "# timestamp feature\n",
    "df_train = date_to_ts(df_train)\n",
    "df_test = date_to_ts(df_test)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_train.loc[:, ['timestamp']])\n",
    "df_train.loc[:, ['timestamp']] = scaler.transform(df_train.loc[:, ['timestamp']])\n",
    "df_test.loc[:, ['timestamp']] = scaler.transform(df_test.loc[:, ['timestamp']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    random_state=42,\n",
    "    max_features=5,\n",
    "    criterion='gini',\n",
    "    min_samples_split=5,\n",
    "    n_jobs=N_JOBS,\n",
    "    verbose=1\n",
    "    )\n",
    "\n",
    "rf_test.fit(df_train, sentiments_train)\n",
    "pred_test = rf_test.predict(df_test)\n",
    "f1_test = f1_score(sentiments_test, pred_test, average='macro')\n",
    "\n",
    "f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "\n",
    "features['Lexicon-based'] = [\n",
    "    '#positive_ngrams',\n",
    "    '#negative_ngrams',\n",
    "    'sum_positive_entropies',\n",
    "    'sum_negative_entropies',\n",
    "    'last_word_polarity'\n",
    "    ]\n",
    "    \n",
    "features['PoS-oriented'] = [\n",
    "    'so_sum(NOUN)',\n",
    "    'so_sum(ADJ)',\n",
    "    'so_sum(VERB)',\n",
    "    'so_sum(ADV)',\n",
    "    'so_sum(INTJ)',\n",
    "    'sone_sum(NOUN)',\n",
    "    'sone_sum(ADJ)',\n",
    "    'sone_sum(VERB)',\n",
    "    'sone_sum(ADV)',\n",
    "    'sone_sum(INTJ)'\n",
    "    ]\n",
    "\n",
    "features['Twitter-specific'] = [\n",
    "    '#urls',\n",
    "    '#hashtags',\n",
    "    '#targets'\n",
    "    ]\n",
    "\n",
    "features['Textual'] = [\n",
    "    '#capitalized_words',\n",
    "    '#words',\n",
    "    'avg_words_length',\n",
    "    '#question_marks',\n",
    "    '#esclamation_marks',\n",
    "    '#quotes'\n",
    "    ]\n",
    "\n",
    "features['Non-Textual'] = [\n",
    "    'timestamp',\n",
    "    '#tweets',\n",
    "    'avg_sentiment'\n",
    "    ]\n",
    "    \n",
    "features['BoW'] = vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove subsets of features once at time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "results = []\n",
    "\n",
    "for f, list in features.items():\n",
    "    df_train_subset = df_train.drop(list, axis=1)\n",
    "    df_test_subset = df_test.drop(list, axis=1)\n",
    "\n",
    "    rf_subset = RandomForestClassifier(n_estimators=100, n_jobs=N_JOBS, random_state=42)\n",
    "    rf_subset.fit(df_train_subset, sentiments_train)\n",
    "    pred_subset = rf_subset.predict(df_test_subset)\n",
    "    score = f1_score(sentiments_test, pred_subset, average='macro')\n",
    "    results.append((f, score))\n",
    "\n",
    "rf_subset = RandomForestClassifier(n_estimators=100, n_jobs=N_JOBS, random_state=42)\n",
    "rf_subset.fit(df_train, sentiments_train)\n",
    "pred_subset = rf_subset.predict(df_test)\n",
    "score = f1_score(sentiments_test, pred_subset, average='macro')\n",
    "results.append(('all features', score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df_plot = pd.DataFrame({'F1-Macro Score' : [x[1] for x in results], 'Features group removed' : [x[0] for x in results]})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0,1)\n",
    "plt.grid()\n",
    "sns.barplot(x=[x[1] for x in results], y=[x[0] for x in results], orient='h', color='tab:blue')\n",
    "chart = sns.barplot(data=df_plot, x='F1-Macro Score', y='Features group removed', orient='h', color='tab:blue', ci=None)\n",
    "chart.bar_label(chart.containers[0])\n",
    "\n",
    "plt.savefig('bars.svg', format='svg', pad_inches=50, dpi=300, quality=80, optimize=True, progressive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% of development set is used\n",
    "\n",
    "# user features\n",
    "df_dev, user_map, default_values = user_features(df_dev) # fit\n",
    "df_eval, _, _ = user_features(df_eval, user_map, default_values) # transform\n",
    "\n",
    "sentiments_dev = df_dev.iloc[:, 0]\n",
    "df_dev = df_dev.iloc[:, 1:]\n",
    "\n",
    "# lexicon features\n",
    "lexicon = LexiconExtractor(df_dev, sentiments_dev, df_eval)\n",
    "df_dev, df_eval = lexicon.get_DataFrames()\n",
    "so, ne = lexicon.get_so_ne()\n",
    "\n",
    "df_dev_text = df_dev['text']\n",
    "df_eval_text = df_eval['text']\n",
    "df_dev.drop(['text'], axis=1, inplace=True)\n",
    "df_eval.drop(['text'], axis=1, inplace=True)\n",
    "\n",
    "# pos oriented features\n",
    "df_dev = add_pos_features(df_dev, df_dev_text, so, ne)\n",
    "df_eval = add_pos_features(df_eval, df_eval_text, so, ne)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# BoW features\n",
    "dev_docs = [' '.join([t for t, s in zip(df_dev_text, sentiments_dev) if s==i]) for i in [0, 1]]\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=123, binary=False)\n",
    "tfidf.fit(dev_docs)\n",
    "vocab = tfidf.vocabulary_\n",
    "tfidf_dev = tfidf.transform(df_dev_text)\n",
    "tfidf_eval = tfidf.transform(df_eval_text)\n",
    "df_dev = pd.concat([df_dev, pd.DataFrame.sparse.from_spmatrix(tfidf_dev, index=df_dev.index, columns=sorted(vocab.keys()))], axis=1)\n",
    "df_eval = pd.concat([df_eval, pd.DataFrame.sparse.from_spmatrix(tfidf_eval, index=df_eval.index, columns=sorted(vocab.keys()))], axis=1)\n",
    "\n",
    "# timestamp feature\n",
    "df_dev = date_to_ts(df_dev)\n",
    "df_eval = date_to_ts(df_eval)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_dev.loc[:, ['timestamp']])\n",
    "df_dev.loc[:, ['timestamp']] = scaler.transform(df_dev.loc[:, ['timestamp']])\n",
    "df_eval.loc[:, ['timestamp']] = scaler.transform(df_eval.loc[:, ['timestamp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_eval = RandomForestClassifier(\n",
    "\tn_estimators=5000,\n",
    "\trandom_state=42,\n",
    "\tmax_features=5,\n",
    "\tcriterion='gini',\n",
    "\tmin_samples_split=5,\n",
    "\tn_jobs=N_JOBS,\n",
    "\tverbose=1\n",
    "\t)\n",
    "\n",
    "rf_eval.fit(df_dev, sentiments_dev)\n",
    "pred_eval = rf_eval.predict(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "out = [(i, p) for (i, p) in enumerate(pred_eval)]\n",
    "\n",
    "OUT_FILE = 'out_20_01.csv'\n",
    "\n",
    "with open(OUT_FILE, 'w') as f:\n",
    "\twriter = csv.writer(f)\n",
    "\twriter.writerow([\"Id\", \"Predicted\"])\n",
    "\tfor row in out:\n",
    "\t\twriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "freq = lexicon.get_freq()\n",
    "words = lexicon.get_words()\n",
    "\n",
    "sone = {k:so[k]*ne[k] for k in so.keys()}\n",
    "stopWords = set(stopwords.words('english'))\n",
    "word_list = words.words()\n",
    "best_words = sorted([(k,v*so[k]) for k, v in ne.items() if freq[k] > 2000 and k not in stopWords and k in word_list], key=lambda x: abs(x[1]), reverse=True)[:20]\n",
    "w = [i[0] for i in best_words]\n",
    "values = [i[1] for i in best_words]\n",
    "\n",
    "red = np.array([.84, .15, .16])\n",
    "blue = np.array([.12, .47, .71])\n",
    "white = np.array([1, 1, 1])\n",
    "\n",
    "sns.barplot(y=w, x=values, orient='h', palette=[red*abs(v*2) + white*(1-abs(2*v)) if v > 0 else blue*abs(2*v) + white*(1-abs(v*2)) for v in values])\n",
    "\n",
    "# plt.savefig('fig.eps', format='eps')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
